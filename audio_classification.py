# -*- coding: utf-8 -*-
"""Audio_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/122DpGdHgmxy-J6kJ2_8umXNg9ZEez_gF

Audio Classifier
=========================

## Important information

* Choose a group name/number and rename the file with it.
* Be sure to enter the names of every member in the group in the cell below.
* Use the power of number and work together as a team.
* You can employ the use of the HPC (high perfomance computer) provided your server account has been established.
* The tutor is always available to give more explanation and assistance whenever/wherever required, but make sure you have made effort yourself.
* Please remove all test codes.
* The solutions to the tasks should be turn in on Friday 13th Dec, end of the day (12am). 
* A notebook submission is required per group, no multiple submissions from same group. Submit by attaching only the .ipynb file please, not zip and I don't need other files (e.g the data) I already have them.

This task is inspired by the Audio Classifier Tutorial (found [here](https://pytorch.org/tutorials/beginner/audio_classifier_tutorial.html?highlight=audio))
**Author**: `Winston Herring <https://github.com/winston6>`_

This tutorial will show you how to correctly format an audio dataset and
then train/test an audio classifier network on the dataset. First, let’s
import the common torch packages as well as ``torch``, ``torchaudio``, ``pandas``,
and ``numpy``. ``torchaudio`` is available `here <https://github.com/pytorch/audio>`_
and can be installed by following the
instructions on the website but I encountered a lot of trouble installing it, this ` conda install -c pytorch torchaudio-cpu ` rather worked for me.

If you have a CUDA GPU use that. Running
the network on a GPU will greatly decrease the training/testing runtime.

1: Importing the Dataset
---------------------

We will use the UrbanSound8K dataset to train our network. It is
available for free `here <https://urbansounddataset.weebly.com/>`_ and contains
10 audio classes with over 8000 audio samples! Once you have downloaded
the compressed dataset, extract it to your current working directory.
First, we will look at the csv file that provides information about the
individual sound files. ``pandas`` allows us to open the csv file and
use ``.iloc()`` to access the data within it.

The 10 audio classes in the UrbanSound8K dataset are air_conditioner,
car_horn, children_playing, dog_bark, drilling, enginge_idling,
gun_shot, jackhammer, siren, and street_music. Let’s play a couple files
and see what they sound like. The first file is street music and the
second is an air conditioner.

2: Formatting the Data
-------------------

Now that we know the format of the csv file entries, we can construct
our dataset. We will create a rapper class for our dataset using
``torch.utils.data.Dataset`` that will handle loading the files and
performing some formatting steps. The UrbanSound8K dataset is separated
into 10 folders. We will use the data from 9 of these folders to train
our network and then use the 10th folder to test the network. The rapper
class will store the file names, labels, and folder numbers of the audio
files in the inputted folder list when initialized. The actual loading
and formatting steps will happen in the access function ``__getitem__``.

In ``__getitem__``, we use ``torchaudio.load()`` to convert the wav
files to tensors. ``torchaudio.load()`` returns a tuple containing the
newly created tensor along with the sampling frequency of the audio file
(44.1kHz for UrbanSound8K). The dataset uses two channels for audio so
we will use ``torchaudio.transforms.DownmixMono()`` (not available in the latest version of `torchaudio`) to convert the audio
data to one channel. Next, we need to format the audio data. The network
we will make takes an input size of 32,000, while most of the audio
files have well over 100,000 samples. The UrbanSound8K audio is sampled
at 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By
downsampling the audio to aproximately 8kHz, we can represent 4 seconds
with the 32,000 samples. This downsampling is achieved by taking every
fifth sample of the original audio tensor. Not every audio tensor is
long enough to handle the downsampling so these tensors will need to be
padded with zeros. The minimum length that won’t require padding is
160,000 samples.
"""

!pip3 install torchaudio

import torch
import torchaudio

"""3: Define the Network
------------------

For this task we want to closely reproduce the achitectures described in https://arxiv.org/pdf/1610.00087.pdf. You task is to read extensively the paper and reproduce the achitectures <font color='green'> M3, M5, M11 and M18. The M34-res is a bonus.</font>
While attempting to reproduce the architectures endeavour to read through the common [pitfalls](https://urbansounddataset.weebly.com/urbansound8k.html#10foldCV) to get it right.
"""

#upload dataset from the web site
!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz

#unzip the dataset
!tar -xf UrbanSound8K.tar.gz

"""We will use the same optimization technique used in the paper, an Adam
optimizer with weight decay set to 0.0001. At first, we will train with
a learning rate of 0.01, but we will use a ``scheduler`` to decrease it
to 0.001 during training.

4: Training and Testing the Network
--------------------------------

You can define a training function that will feed our training data into the model and perform the backward pass and optimization steps. You can also make one for testing the networks accuracy and set the model to ``eval()`` mode and then run inference on the test dataset. Calling ``eval()`` sets the training variable in all modules in the network to false. Certain layers like batch normalization and dropout layers behave differently during training so this step is crucial for getting correct results.

Finally, we can train and test the network. Train the network for as many epochs as time allows you. The network will be tested after each epoch to see how the accuracy varies during the training.

Conclusion
----------

If trained on 9 folders, the network should be about 40% accurate by the end of the training process for the least possible epochs. Training on less folders will result in a lower overall accuracy. Greater accuracies can be achieved using deeper CNNs at the expense of a larger memory footprint.

For more advanced audio applications, such as speech recognition,
recurrent neural networks (RNNs) are commonly used. There are also other
data preprocessing methods, such as finding the mel frequency cepstral
coefficients (MFCC), that can reduce the size of the dataset.
"""

import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torch.nn as nn

#rapper for the UrbanSound8K dataset

class AudioDataset(Dataset):
    """
    A rapper class for the UrbanSound8K dataset.
    """

    def __init__(self, file_path, audio_paths, folds):
        """
        Args:
            file_path(string): path to the audio csv file
            root_dir(string): directory with all the audio folds
            folds: integer corresponding to audio fold number or list of fold number if more than one fold is needed
        """
        self.audio_file = pd.read_csv(file_path)
        self.folds = folds
        self.audio_paths = glob.glob(audio_paths + '/*' + str(self.folds) + '/*')
    
    

    def __len__(self):
        return len(self.audio_paths)

    def __getitem__(self, idx):
        
        audio_path = self.audio_paths[idx]
        audio, rate = torchaudio.load(audio_path, normalization=True)
        
        audio = audio.mean(0, keepdim=True) #transfrming the data into one chanel 
        c, n = audio.shape
        zero_need = 160000 - n
        audio_new = F.pad(audio, (zero_need //2, zero_need //2), 'constant', 0)
        audio_new = audio_new[:,::5]
        
        #Getting the corresponding label
        audio_name = audio_path.split(sep='/')[-1]
        labels = self.audio_file.loc[self.audio_file.slice_file_name == audio_name].iloc[0,-2]
        
        return audio_new, labels

AudioDataset?

#path of the dataset in the drive
file_path = '/content/UrbanSound8K/metadata/UrbanSound8K.csv'
audio_paths = '/content/UrbanSound8K/audio'

##Here we import csv file

label_data = pd.read_csv(file_path)

label_data.head(6)

##scatter plot of the started and ended point of each record.

a, b = label_data['start'], label_data['end']

import matplotlib.pyplot as plt
plt.scatter(a,b)
plt.show()



##Example of filter for exploiring the data

fil = torch.FloatTensor([1,0,0,0,1,1,0,1,0] )
audio = torch.zeros(1, 32000)
fil.unsqueeze(0).unsqueeze(0).shape,audio[0].unsqueeze(0).unsqueeze(0).shape

print(F.conv1d(audio.unsqueeze(0), fil.unsqueeze(0).unsqueeze(0)).shape)

"""## Models for each arithecture

**We define here the training, test and accuracy functions for all the architectures.**
"""

def G_training(n, mod, name):
  '''
  n : number of epochs
  mod: model used for training
  name: name of the model for clarification
  Output: Just graph of loss and accuracy for taining and test dataset. But we can make more beautiful by returning the list of loss and accuracy.
  '''

    lis_folder = [1,2,3,4,5,6,7,8,9,10]
    _lr_ = 0.01
    for i in range(1,11):
        
        lis = list(lis_folder)
        lis.remove(i)
        ##Here we upload all the dataset for training and testing model
        _audio_trainset = AudioDataset(file_path, audio_paths, lis)
        _audio_testset = AudioDataset(file_path, audio_paths, i)
        ## transformation of data in batchsize
        train_loader = torch.utils.data.DataLoader(_audio_trainset, batch_size= 128, shuffle=False, num_workers=10)
        test_loader = torch.utils.data.DataLoader(_audio_testset, batch_size=128, shuffle=False, num_workers=10)
        print('CrossValidation number {} for the Model {}'.format(i,name) )
        #set device to cuda()
        device = 'cuda:0'
        #Metric to compute the loss
        criterion = nn.CrossEntropyLoss()
        #Optimizer
        optimizer = torch.optim.Adam(mod.parameters(), lr=_lr_, weight_decay=1e-4)
        #scheduler
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 5, gamma = 0.1)

        loss_train = []
        loss_test = []
        acc_train = []
        acc_test = []
        for epochs in range(n):
            mod.train()
            correct_train = 0
            loss_1 = []
            ##training part.
            for batch_id, (batch, target) in enumerate(train_loader):
                #cuda
                batch, target, = batch.to(device), target.to(device)
                #Zero grad
                optimizer.zero_grad()
                #forward through the model
                output = mod(batch)
                #compute the loss
                loss = criterion(output, target)
                #backward 
                loss.backward()
                #Gradient step
                optimizer.step()
                #loss
                loss_1.append(loss)

                # accuracity
                correct_train += accuracy(output, target)[0].item()
            print('Epoch: =={}'.format(epochs))
            print('=====================> train accuracy: {}'.format((correct_train/len(train_loader.dataset))*100) ) 
            print('=====================> train loss: {}'.format(torch.mean(torch.tensor(loss_1))))
            acc_train.append((correct_train/len(train_loader.dataset))*100)
            loss_train.append(torch.mean(torch.tensor(loss_1)))

            # evaluate on the test set  
            with torch.no_grad():
                correct_test = 0
                loss_2 = []
                mod.eval()
                for batch_id, (batch, target) in enumerate(test_loader):
                    #cuda
                    batch, target = batch.to(device), target.to(device)
                    #forward through the model
                    output = mod(batch)
                  #loss
                    loss = criterion(output, target) 

                    loss_2.append(loss)
                  #Gradient step
                    optimizer.step()
                  # get the index of the max log-probability
                    correct_test += accuracy(output, target)[0].item()
                #Scheduler
                scheduler.step()

                print('=====================> test accuracy: {}'.format( (correct_test/len(test_loader.dataset))*100) )
                print('=====================> test loss: {}'.format( torch.mean(torch.tensor(loss_2)) ))
                acc_test.append(correct_test/len(test_loader.dataset))
                loss_test.append(torch.mean(torch.tensor(loss_2)))

          #Plot training loss and accuracy corresponding to each learning rate
        plt.figure(dpi=100, figsize=(9,3))
        plt.subplot(121)
        plt.plot( loss_train, label = 'loss_train')
        plt.plot( loss_test, label = 'loss_test')
        plt.legend()
        plt.title("loss and acc of Train for {} cross {} lr= {}".format(name,i,_lr_))
        plt.show()
          #Plot test loss and accuracy corresponding to each learning rate
        plt.subplot(122)
        plt.plot(acc_train, label = 'acc_train')
        plt.plot( acc_test, label = 'acc_test')
        plt.legend()
        plt.title("loss and acc of test for {} cross {} lr= {}".format(name,i,_lr_))
        plt.show()
        plt.savefig('test_Cross_and-train{}_{}.png'.format(i, name))
  return #loss_train, loss_test, acc_train, acc_test
  
  
  
  ##accuracity function
def accuracy(output, target, topk=(1,)):
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res

"""**Frist model: M3**"""

## M3 architheture#
class M3(nn.Module):
    
    def __init__(self):
        super(M3, self).__init__()
        self.conv1 =  nn.Conv1d(1, 256, kernel_size=80, stride=4, padding=38)
        self.conv2 =  nn.Conv1d(256, 256, kernel_size = 3, padding=1)
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(256)
        self.averpool = nn.AdaptiveAvgPool1d(1)
        self.pool = nn.MaxPool1d(4)
        self.fc = nn.Linear(256,10)
        
    def forward(self,x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.bn1(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.bn2(x)
        x = self.pool(x)
        x = self.averpool(x)
        x = x.view(-1, 256)
        x = self.fc(x)
        
        return F.softmax(x, dim=1)
               
m3 = M3()
m3(audio.unsqueeze(0))

"""**Application of M3**"""

model3 = M3().cuda()
G_training(10,model3, 'M3')

"""**Second architecture, M5**"""

class M5(nn.Module):
    
    def __init__(self):
        super(M5, self).__init__()
        #fully convolutional
        self.conv1 =  nn.Conv1d(1, 256, kernel_size=80, stride=4, padding=38)
        self.conv2 =  nn.Conv1d(256, 128, kernel_size = 3, padding=1)
        self.conv3 =  nn.Conv1d(128, 256, kernel_size = 3, padding=1)
        self.conv4 =  nn.Conv1d(256, 512, kernel_size = 3, padding=1)
        
        #batch normalization
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(256)
        self.bn4 = nn.BatchNorm1d(512)
        
        #Max averaged pooling
        self.averpool = nn.AdaptiveAvgPool1d(1)
        #max pooling
        self.pool = nn.MaxPool1d(4)
        #linear layer
        self.fc = nn.Linear(512,10)
        
    def forward(self,x):
        #First layer 
        x = self.bn1(F.relu(self.conv1(x)))
        x = self.pool(x)
        #Second layer
        x = self.bn2(F.relu(self.conv2(x)))
        x = self.pool(x)
        #Thrid layer
        x = self.bn3(F.relu(self.conv3(x)))
        x = self.pool(x)
        #Fourth layer
        x = self.bn4(F.relu(self.conv4(x)))
        x = self.pool(x)
        #max averaged pooling
        x = self.averpool(x)      
        #flatten
        x = x.view(-1, 512)
        x = self.fc(x)
        return F.softmax(x, dim=1)
m5 = M5().cuda()
m5(audio.unsqueeze(0))

model5 = M5().cuda()
G_training(2,model5, 'M5')

class M11(nn.Module):
    
    def __init__(self):
        super(M11, self).__init__()
        #fully convolutional
        self.conv1 =  nn.Conv1d(1, 64, kernel_size=80, stride=4, padding=38)
        self.conv2 =  nn.Conv1d(64, 64, kernel_size = 3, padding=1)
        self.conv3 =  nn.Conv1d(64, 128, kernel_size = 3, padding=1)
        self.conv31 =  nn.Conv1d(128, 128, kernel_size = 3, padding=1)
        self.conv4 =  nn.Conv1d(128, 256, kernel_size = 3, padding=1)
        self.conv41 =  nn.Conv1d(256, 256, kernel_size = 3, padding=1)
        self.conv5 =  nn.Conv1d(256, 512, kernel_size = 3, padding=1)
        self.conv51 =  nn.Conv1d(512, 512, kernel_size = 3, padding=1)

        
        #batch normalization
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(64)
        self.bn3 = nn.BatchNorm1d(128)
        self.bn4 = nn.BatchNorm1d(256)
        self.bn5 = nn.BatchNorm1d(512)
        
        #Max averaged pooling
        self.averpool = nn.AdaptiveAvgPool1d(1)
        #max pooling
        self.pool = nn.MaxPool1d(4)
        #linear layer
        self.fc = nn.Linear(512,10)
        
    def forward(self,x):
        #First layer
        x = self.bn1(F.relu(self.conv1(x)))
        x = self.pool(x)
        #Second layer *2
        for i in range(2):
            x = self.bn2(F.relu(self.conv2(x)))
        x = self.pool(x)
        # #Thrid layer *2
        x = self.bn3(F.relu(self.conv3(x)))
        x = self.bn3(F.relu(self.conv31(x)))
        x = self.pool(x)
        # #Fourth layer *3
        x = self.bn4(F.relu(self.conv4(x)))
        for i in range(2):
            x = self.bn4(F.relu(self.conv41(x)))
        x = self.pool(x)
        #Fifth layer *2
        x = self.bn5(F.relu(self.conv5(x)))
        x = self.bn5(F.relu(self.conv51(x)))
        x = self.pool(x)
        #max averaged pooling
        x = self.averpool(x)
        #flatten
        x = x.view(-1, 512)
        x = self.fc(x)
        return F.softmax(x, dim=1)

model11 = M11()
model11(audio.unsqueeze(0))

model11 = M11().cuda()
G_training(10,model11, 'M11')

class M18(nn.Module):
    
    def __init__(self):
        super(M18, self).__init__()
        #fully convolutional
        self.conv1 =  nn.Conv1d(1, 64, kernel_size=80, stride=4, padding=38)
        self.conv2 =  nn.Conv1d(64, 64, kernel_size = 3, padding=1)
        self.conv3 =  nn.Conv1d(64, 128, kernel_size = 3, padding=1)
        self.conv31 =  nn.Conv1d(128, 128, kernel_size = 3, padding=1)
        self.conv4 =  nn.Conv1d(128, 256, kernel_size = 3, padding=1)
        self.conv41 =  nn.Conv1d(256, 256, kernel_size = 3, padding=1)
        self.conv5 =  nn.Conv1d(256, 512, kernel_size = 3, padding=1)
        self.conv51 =  nn.Conv1d(512, 512, kernel_size = 3, padding=1)

        
        #batch normalization
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(64)
        self.bn3 = nn.BatchNorm1d(128)
        self.bn4 = nn.BatchNorm1d(256)
        self.bn5 = nn.BatchNorm1d(512)
        
        #Max averaged pooling
        self.averpool = nn.AdaptiveAvgPool1d(1)
        #max pooling
        self.pool = nn.MaxPool1d(4)
        #linear layer
        self.fc = nn.Linear(512,10)
        
    def forward(self,x):
        #First layer
        x = self.bn1(F.relu(self.conv1(x)))
        x = self.pool(x)
        #Second layer *4
        for i in range(4):
            x = self.bn2(F.relu(self.conv2(x)))
        x = self.pool(x)
        # #Thrid layer *4
        x = self.bn3(F.relu(self.conv3(x)))
        for i in range(3):
            self.bn3(F.relu(self.conv31(x)))
        x = self.pool(x)
        # #Fourth layer *4
        x = self.bn4(F.relu(self.conv4(x)))
        for i in range(3):
            x = self.bn4(F.relu(self.conv41(x)))
        x = self.pool(x)
        #Fifth layer *
        x = self.bn5(F.relu(self.conv5(x)))
        for i in range(3):
            x = self.bn5(F.relu(self.conv51(x)))
        x = self.pool(x)
        #max averaged pooling
        x = self.averpool(x)
        #flatten
        x = x.view(-1, 512)
        x = self.fc(x)      
        return F.softmax(x, dim=1)

model18 = M18()
model18(audio.unsqueeze(0))

model18 = M18().cuda()
G_training(10,model18, 'M18')

"""# **Implementation of the residual network 34 in the Very deep convolutional neural network**"""

class ResNetModule(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super(ResNetModule, self).__init__()
        
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)
        self.conv2  = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)
        self.batch = nn.BatchNorm1d(out_channels)
        self.relu  = nn.ReLU()
        
        
        
    def forward(self,x):
      #first fully convolution
        x1 = self.conv1(x)
        x1 = self.batch(x1)
        x1 = self.relu(x1)
        #second fully convolution
        x1 = self.conv2(x1)
        x1 = self.batch(x1)
        
        # print(x1.shape, x.shape)
        if x1.shape[1] != x.shape[1]:
            x = x.repeat(1,2,1) # repeat with 2 by 2
        
        x = x1 + x
        x = self.batch(x)
        x = self.relu(x)
        return x
        
M34res = nn.Sequential(
    #conv1
    nn.Conv1d(1,48,kernel_size=80, stride=4, padding=38),
    nn.BatchNorm1d(48),
    nn.ReLU(),
 
    nn.MaxPool1d(4),
    
    ResNetModule(48,48,3,padding=1),
    ResNetModule(48,48,3,padding=1),
    ResNetModule(48,48,3,padding=1),
    
    nn.MaxPool1d(4),
    
    ResNetModule(48,96,3,padding=1),
    ResNetModule(96,96,3,padding=1),
    ResNetModule(96,96,3,padding=1),
    ResNetModule(96,96,3,padding=1),
    
    nn.MaxPool1d(4),
    
    ResNetModule(96,192,3,padding=1),
    ResNetModule(192,192,3,padding=1),
    ResNetModule(192,192,3,padding=1),
    ResNetModule(192,192,3,padding=1),
    ResNetModule(192,192,3,padding=1),
    ResNetModule(192,192,3,padding=1),
    
    nn.MaxPool1d(4),
    
    ResNetModule(192,384,3,padding=1),
    ResNetModule(384,384,3,padding=1),
    ResNetModule(384,384,3,padding=1),
    
    nn.AdaptiveAvgPool1d(1),
    nn.Flatten(),
    nn.Linear(384,10)
)
# audio = torch.zeros(1,32000)
# M34res(audio.unsqueeze(0)).shape

M34res_model =  M34res.cuda()

G_training(10,M34res_model, 'M34res')

